\chapter{Background}
\label{chap:background}

Nowadays, the complexity of software systems is increasing. As we develop more and more functionality in safety oriented solutions, we can reach a complexity level, where formal verification isn't enough on it's own to assure a specified behavior. With the introduction of Internet of Things (\iot), and new mobile communication technologies, systems are becoming dynamic, and this transition needs new embedded monitoring solutions.

\section{Model driven software developement}

Using models to design complex systems is essential in traditional engineering disciplines. Models help us understand a complex problem and its potential solutions through abstraction. Therefore, it seems obvious that software systems, which are often among the most complex engineering systems, can benefit greatly from using models and modeling techniques. \citep{pastor2008model}

In safety--critical systems, model driven software development is mandatory. In these systems, e.g. requirement traceability is necessity. Without a model based software design process, the traceability of requirements and the development process of the components are hard to follow. In a traditional software development process, these documents would have been written by natural language on paper, or some electronically document format. Finding tracing information between documents in a complex system is almost impossible.

Model based software development consists two main components:
\begin{itemize}
	\item Tools: Tools allow the developers to create models, e.g. UML, EMF, SysML
	\item Process: The process describes how to use tools in a development process, e.g. SYSMOD, RUP, Y model
\end{itemize}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}[
			start chain=going below,
			squarednode/.style={
				rectangle,
				draw=black,
				very thick,
				minimum height=1.5cm,
				text width=2.5cm,
				inner sep=2mm,
				align=center
			},
			every loop/.style={
				looseness=2,
				text width=2cm
			}
		]
		\node[squarednode] (D1) {System\\Design Model};
		\node[squarednode] (D2) [below = of D1, xshift = .5cm] {Architecture\\Design Model};
		\node[squarednode] (D3) [below = of D2, xshift = .5cm] {Component\\Design Model};
		\node[squarednode] (V1) [right = 3cm of D1] {System\\V\&V Model};
		\node[squarednode] (V2) [below = of V1, xshift = -.5cm]{Architecture\\V\&V Model};
		\node[squarednode] (V3) [below = of V2, xshift = -.5cm] {Component\\V\&V Model};

		\node[squarednode] (GC) at ($(D3)!0.5!(V3)$) [yshift=-3.5cm, text width=5.5cm] {Design + V\&V Artifacts\\(Source code, Glue code,\\Config. Tables, Test Cases,\\Monitors, Fault Trees, etc.)};

		\draw[thick,->] (D3.south) -- (GC.north -| D3) node [midway, left, align=right] {Code generation};
		\draw[thick,->] (V3.south) -- (GC.north -| V3) node [midway, right, align=left] {Test generation};

		\draw[thick,->] (D1) edge [loop left, align=right] node {Design rules} ();
		\draw[thick,->] (D2) edge [loop left, align=right] node {Design rules} ();
		\draw[thick,->] (D3) edge [loop left, align=right] node {Design rules} ();

		\draw[thick,->] (V1) edge [loop right, align=left] node {Formal methods} ();
		\draw[thick,->] (V2) edge [loop right, align=left] node {Formal methods} ();
		\draw[thick,->] (V3) edge [loop right, align=left] node {Formal methods} ();

		\draw[thick,->] (D1.south) -- (D2.north) node [midway, left, align=right] {Refine};
		\draw[thick,->] (D2.south) -- (D3.north) node [midway, left, align=right] {Refine};
		\draw[thick,<-] (V1.south) -- (V2.north) node [midway, right, align=left] {Use};
		\draw[thick,<-] (V2.south) -- (V3.north) node [midway, right, align=left] {Use};

		\draw[thick,<->] (D1.east) -- (V1.west);
		\draw[thick,<->] (D2.east) -- (V2.west);
		\draw[thick,<->] (D3.east) -- (V3.west);

	\end{tikzpicture}
	\caption{Diagram of the Y model}
	\label{fig:y_model}
\end{figure}

\subsection{Y model}

The Y model (\cref{fig:y_model}) depicts a basic workflow of model based software development. Every design level has it's own design rules it must conform to. The detailed levels of the system must be traceable to the upper levels. The artifact of the design process is the generated code, and generated test cases.

\section{Verification techniques}

\subsection{Design time verification}

Design time verification, or formal verification is the analysis of a software in the design process. It's mostly based around model driven system development, where we specify the entire software\,---and hardware if needed---\,as a model. On this model, we can run analysis to ensure that software always behaves according to the specification. Formal verification is a commonly used process in the verification of safety critical systems, or networking protocols.

As mentioned, dynamic behavior takes significant part of a modern software system, even in a safety critical cases. As the availability of sensor networks and advanced communication technologies spreads, distributed components pose a challenge in the verification process.

While formally verifying a software, the software engineer builds a model, and tests this model against given scenarios, searching for incorrect behavior. But the engineer is the responsible for the specification of the scenarios, and not all corner cases are obvious. This method can lead situations, where the system may hide a potential hazard, but because no verification indicated incorrect behavior, we find the system verified.

\subsection{Runtime verification}

On behalf of design time verification which is performed while developing the software components, runtime verification checks the systems behavior while operating. This is beneficial because if an erroneous state in the software is not found at the formal verification stage of development, runtime verification techniques can still detect the faulty state of the software, and raise an error. This error can be detected by other components, and\,---if possible\,---resolve this error.

Runtime verification components can be implemented in form of monitors. The monitors' responsibility is to take signals from a software, and compare it to a simplified model inside of the monitor.

Software components can communicate with the monitor via multiple ways. The communication method implies where the monitor is placed.
\begin{enumerate}
	\item Directly by calling the monitors' methods. In this case the monitors' methods are called from the monitored process itself. The monitor is placed in the same compilation unit.
	      \item\label{item:agent} Remotely by an agent. An embedded agent component is called from the monitored components, or the agent is actively extracting the monitored softwares' state. The agent then forwards this information to the monitor. The monitor can be virtually everywhere.
\end{enumerate}

... (Itt kellene arról beszélni, hogy mi lesz a case study-ban)

\section{Complex event processing}

Complex event processing (\cep) is a set of techniques and tools to help us understand and control event-driven information systems.\citep{Luckham:2001:PEI:515781}

Complex event processing frameworks exists because of the problem of processing large amount of events coming inside into an application. While its easy to search  very simple patterns inside an incoming on--line stream of information, e.g. the same thing happening $x$ times after each other, at the moment the developer wants to find more complex, maybe temporal pattern in this incoming information, this problems grows into a nontrivial one.

For example take an example from the financial world. A bank wants to avoid fraud with it's clients credit card. The bank receives a huge amount of payment information daily, and wants to filter
\begin{itemize}
	\item The same person's credit card
	\item With some temporal property e.g. in a day
	\item Used somewhere outside from the country the last 2 payments was done
\end{itemize}

With the capability to find such specific occasions, complex event processor is great for monitoring purposes. This thesis will show the usage of this monitoring capability with very limited resources. The basic workflow for monitoring with \cep is illustrated in \cref{fig:cep_monitoring}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}[
			start chain=going below,
			squarednode/.style={
				rectangle,
				draw=black,
				very thick,
				minimum height=1cm,
				minimum width=2.5cm,
				inner sep=.2cm,
				on chain
			},
		]
		\node[squarednode] (A) {High level specification};
		\node[squarednode] (B) {Pattern creation};
		\node[squarednode] (C) {Generation of monitor};
		\node[squarednode] (D) {Monitor execution};

		\draw[thick, ->] (A) -- (B);
		\draw[thick, ->] (B) -- (C);
		\draw[thick, ->] (C) -- (D);

	\end{tikzpicture}
	\caption{A \cep based monitoring solution}
	\label{fig:cep_monitoring}
\end{figure}

\subsection{Event}

An event is an instance of an event type, which is distinguished by an unique name.
Event is the basic and most important aspect of a complex event processor. Events can be classified into two main categories:
\begin{itemize}
	\item Atomic event. These events marks a point in time, and cannot be broken down to other events.
	\item Complex event. A complex event can be caused by some pattern consisting atomic, and other complex events.
\end{itemize}

\needspace{10em}
\subsection{Event streams}
\label{subsection:event_streams}
The input of a \cep are streams of atomic events. These atomic events can come from various sources, as follows.
\begin{enumerate}
	\item Direct event stream handling. This is an imperative approach, because the events pushed programmatically from code through an API.

	\item Model change events. The application have an underlying model and framework, and it can notify the event processor in case of various changes (deletion, insertion, update) of the model.\label{item:model}

	\item Databases. This is a special case of \cref{item:model}, where the model is not implemented in the application, but stored in a database.
\end{enumerate}
\vspace{1ex}
If an \cep supports it, it is possible to mix input streams, and rules can interconnect these event sources.

\subsection{Patterns}
Event processors provides formalisms to describe patterns.

The complex nature of the rules comes from the usage of the reference of other patterns. One pattern might depend on the activation of another pattern.

With this functionality, one can build hierarchical system of pattern. Instead of creating one big pattern\,---if its even possible---\,containing all the functionality, divided smaller, reusable patterns can be written.

\subsection{Engine}

A \cep is built around it's execution engine. The engine accepts the input stream(s), and output is the matches of the patterns. In monitoring situations the output of engine is the event of an erroneous state of the monitored process (\cref{fig:cep_monitoring_event_flow}).

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}[
			start chain=going below,
			squarednode/.style={
				rectangle,
				draw=black,
				very thick,
				minimum height=1cm,
				minimum width=2.5cm,
				inner sep=.2cm,
				on chain
			},
		]
		\node[squarednode] (A) {Direct stream};
		\node[squarednode] (B) {Model change};
		\node[squarednode] (C) {Database};
		\node[squarednode, minimum height=2cm] (D) [right = 2cm of B] {CEP Engine};

		\draw[thick, ->] (A.east) -- (D);
		\draw[thick, ->] (B.east) -- (D);
		\draw[thick, ->] (C.east) -- (D);
		\draw[thick, ->] (D.east) -- ++(2,0) node [above, pos = 0.8, text width = 2.5cm] {Error\\signals};

		\draw[thick, ->] (5,0.5) -- ++(2,0) node [pos = 1.5] {Event flow};

	\end{tikzpicture}
	\caption{Event flow in a \cep based monitor}
	\label{fig:cep_monitoring_event_flow}
\end{figure}

\subsection{Conclusion}

Traditionally complex event processors are complex frameworks like databases, and modeling frameworks. The aim of this thesis to generate small, but effective code, and run it on embedded environments, where it's impossible to run a full \cep software stack.

\section{Real--time systems}

Real--time systems have some temporal constraints (deadline), which the system must satisfy. In a real--time system the correctness of behavior not only depend on the control logic, but on the temporal properties of it. These constrains often focused on the response time of the system in case of events.

We can divide real--time systems into two main category: hard and soft real--time.

\subsubsection{Hard real--time}

Hard realtime systems have precisely defined deadlines which the system must suffice. If the system hit a deadline, it is considered a failure. An example of a hard--realtime system is an airbag controller inside a car. After a collision, there is a very narrow time band where the airbag must be deployed. A hard--realtime system must have some proof they can deterministically handle these situations without hitting a deadline.

\subsubsection{Soft real--time}

Soft real--time differ from hard--realtime system in the way they stay correct if they miss the deadline, but operation between time constraints needed for normal functionality. One example is the processing of mouse input. If the processing of mouse input lags, it can be annoying for the user. Nonetheless the functionality stays intact, but the usability seriously degrades.

\subsection{Realtime operating systems}

The type of an operating system is defined by how the scheduler decides which program to run and when. For example, the scheduler used in a multi user operating system (such as Unix) will ensure each user gets a fair amount of the processing time. As another example, the scheduler in a desktop operating system (such as Windows) will try and ensure the computer remains responsive to its user.

The scheduler in a \emph{Real Time Operating System} (\rtos) is designed to provide a predictable\,---normally described as deterministic---\,execution pattern. This is particularly of interest to embedded systems as embedded systems often have real time requirements. A real time requirements is one that specifies that the embedded system must respond to a certain event within a strictly defined time (the deadline). A guarantee to meet real time requirements can only be made if the behavior of the operating system's scheduler can be predicted (and is therefore deterministic).\citep{RTOS}

Most of the safety oriented systems based around realtime operating systems. It is important to understand realtime operating systems exists to provide determinism, and not additional performance. With these response guarantees, the software engineer can be sure the incoming interrupt will be served, and the software components responsible for the processing will get \cpu time between the guaranteed limits.

\section{Mission critical systems}

A mission critical system is which failure can cause severe financial loss, natural disaster, or loss of human lives. Most of the time mission critical systems utilizes hard--realtime systems. Association between mission critical systems, and hard--realtime is common, but there are counterexamples:

One example is a bank transaction software. A transaction execution software does not need to execute transactions in real--time, but if a transaction does not executed correctly, it can mean serious financial loss.

Another example is a printer head. A printer head is a hard real time component, because if it hits the specified deadline, the output will not be correct. A misprinted page from a desktop printer doesn't meet the definition of a mission critical system.
